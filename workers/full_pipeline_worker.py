"""
Full Pipeline Worker –¥–ª—è Turbo Parser
Wordstat ‚Üí Direct ‚Üí Clustering ‚Üí Export
"""

import asyncio
import time
import traceback
from datetime import datetime

from PySide6.QtCore import QThread, Signal


class FullPipelineWorkerThread(QThread):
    """–ü–æ—Ç–æ–∫ –¥–ª—è FULL PIPELINE: Wordstat ‚Üí Direct ‚Üí Clustering"""
    log_signal = Signal(str, str, str, str, str, str, str, str)  # –≤—Ä–µ–º—è, —Ñ—Ä–∞–∑–∞, —á–∞—Å—Ç–æ—Ç–∞, CPC, –ø–æ–∫–∞–∑—ã, –±—é–¥–∂–µ—Ç, –≥—Ä—É–ø–ø–∞, —Å—Ç–∞—Ç—É—Å
    stats_signal = Signal(int, int, int, float, float)  # –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, —É—Å–ø–µ—à–Ω–æ, –æ—à–∏–±–æ–∫, —Å–∫–æ—Ä–æ—Å—Ç—å, –≤—Ä–µ–º—è
    log_message = Signal(str)
    error_signal = Signal(str)
    progress_signal = Signal(int, int, str)  # —Ç–µ–∫—É—â–∏–π, –≤—Å–µ–≥–æ, —ç—Ç–∞–ø
    finished_signal = Signal(bool, str)
    results_ready = Signal(list)  # –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
    
    def __init__(self, queries, region=225):
        super().__init__()
        self.queries = queries
        self.region = region
        self.start_time = None
        self._cancelled = False
        
    def run(self):
        """–ó–∞–ø—É—Å–∫ FULL PIPELINE"""
        self.log_message.emit(f"üöÄ –ó–∞–ø—É—Å–∫ Full Pipeline: {len(self.queries)} —Ñ—Ä–∞–∑")
        self.start_time = time.time()
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        success = False
        message = ""
        
        try:
            results = loop.run_until_complete(self._run_full_pipeline())
            message = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} —Ñ—Ä–∞–∑"
            success = True
            self.results_ready.emit(results)
        except Exception as exc:
            message = f"‚ùå –û—à–∏–±–∫–∞: {exc}"
            self.log_message.emit(traceback.format_exc())
            self.error_signal.emit(str(exc))
        finally:
            duration = time.time() - self.start_time if self.start_time else 0
            self.log_message.emit(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.1f} —Å–µ–∫")
            try:
                loop.run_until_complete(loop.shutdown_asyncgens())
            except:
                pass
            loop.close()
            self.finished_signal.emit(success, message)
    
    async def _run_full_pipeline(self):
        """–ü–æ–ª–Ω—ã–π pipeline: freq ‚Üí budget ‚Üí cluster"""
        from ..services.frequency import parse_batch_wordstat
        from ..services.direct import forecast_batch_direct, merge_freq_and_forecast
        
        total_steps = len(self.queries)
        results = []
        
        # –®–ê–ì 1: –ü–∞—Ä—Å–∏–Ω–≥ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ (Wordstat)
        self.log_message.emit("üìä –≠—Ç–∞–ø 1/3: –ü–∞—Ä—Å–∏–Ω–≥ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ (Wordstat)...")
        self.progress_signal.emit(0, total_steps, "Wordstat")
        
        freq_results = await parse_batch_wordstat(
            self.queries,
            chunk_size=80,
            region=self.region
        )
        
        for i, result in enumerate(freq_results):
            if self._cancelled:
                break
            phrase = result['phrase']
            freq = result['freq']
            self.log_signal.emit(
                datetime.now().strftime("%H:%M:%S"),
                phrase,
                f"{freq:,}",
                "-", "-", "-", "-", "üìä"
            )
            self.progress_signal.emit(i + 1, total_steps, "Wordstat")
            await asyncio.sleep(0.01)  # UI update
        
        if self._cancelled:
            return []
        
        # –®–ê–ì 2: –ü—Ä–æ–≥–Ω–æ–∑ –±—é–¥–∂–µ—Ç–∞ (Direct)
        self.log_message.emit("üí∞ –≠—Ç–∞–ø 2/3: –ü—Ä–æ–≥–Ω–æ–∑ –±—é–¥–∂–µ—Ç–∞ (Direct)...")
        self.progress_signal.emit(0, len(freq_results), "Direct")
        
        forecast_results = await forecast_batch_direct(
            freq_results,
            chunk_size=100,
            region=self.region
        )
        
        # –®–ê–ì 3: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        self.log_message.emit("üîó –≠—Ç–∞–ø 3/3: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞...")
        merged = await merge_freq_and_forecast(freq_results, forecast_results)
        
        # –®–ê–ì 4: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clustered = await self._cluster_phrases(merged)
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–æ–≥ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        for i, result in enumerate(clustered):
            phrase = result.get('phrase', '')
            freq = result.get('freq', 0)
            cpc = result.get('cpc', 0)
            impressions = result.get('impressions', 0)
            budget = result.get('budget', 0)
            stem = result.get('stem', '')
            
            self.log_signal.emit(
                datetime.now().strftime("%H:%M:%S"),
                phrase,
                f"{freq:,}",
                f"{cpc:.2f}",
                f"{impressions:,}",
                f"{budget:.2f}",
                stem[:20],  # First 20 chars
                "‚úÖ"
            )
            self.progress_signal.emit(i + 1, len(clustered), "–ì–æ—Ç–æ–≤–æ")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        elapsed = time.time() - self.start_time
        speed = len(clustered) / elapsed * 60 if elapsed > 0 else 0
        self.stats_signal.emit(len(clustered), len(clustered), 0, speed, elapsed)
        
        return clustered
    
    async def _cluster_phrases(self, data: list) -> list:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç–µ–º–º–∞–º (NLTK)"""
        try:
            from nltk.stem.snowball import SnowballStemmer
            from nltk.corpus import stopwords
            import nltk
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            try:
                nltk.data.find('corpora/stopwords')
            except LookupError:
                nltk.download('stopwords', quiet=True)
            
            stemmer = SnowballStemmer('russian')
            russian_stopwords = set(stopwords.words('russian'))
            
            grouped = {}
            for item in data:
                phrase = item['phrase'].lower()
                words = phrase.split()
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                filtered = [w for w in words if w not in russian_stopwords]
                if not filtered:
                    filtered = words  # –ï—Å–ª–∏ –≤—Å–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –±–µ—Ä–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
                
                # –°—Ç–µ–º–º–∏–Ω–≥ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–ª–æ–≤–∞
                stem = stemmer.stem(filtered[0]) if filtered else phrase
                
                if stem not in grouped:
                    grouped[stem] = []
                
                item['stem'] = stem
                grouped[stem].append(item)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º
            result = []
            for stem, items in grouped.items():
                avg_freq = sum(i.get('freq', 0) for i in items) / len(items)
                total_budget = sum(i.get('budget', 0) for i in items)
                
                for item in items:
                    item['group_size'] = len(items)
                    item['group_avg_freq'] = avg_freq
                    item['group_total_budget'] = total_budget
                    result.append(item)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
            result.sort(key=lambda x: x.get('freq', 0), reverse=True)
            
            return result
            
        except Exception as e:
            self.log_message.emit(f"‚ö† –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            for item in data:
                item['stem'] = '-'
                item['group_size'] = 1
            return data
    
    def cancel(self):
        """–û—Ç–º–µ–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        self._cancelled = True
